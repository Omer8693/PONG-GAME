# ğŸ“ Pong Game â€“ Reinforcement Learning Agents

This project demonstrates how to train AI agents to play a simplified version of Pong using various reinforcement learning algorithms. The environment is custom-built using Pygame and integrates easily with Gym-style interfaces. Training metrics are logged via TensorBoard, and models can be compared using a built-in evaluation script.


## ğŸš€ Features

- ğŸ® Custom Pygame-based Pong environment
- ğŸ§  RL algorithms implemented:
  - REINFORCE (Policy Gradient)
  - A2C (Advantage Actor-Critic)
  - Expected SARSA
- ğŸ“ˆ Training logs and metrics via TensorBoard
- ğŸ“Š Evaluation and comparison across models
- ğŸ§ª Simple and Complex environment modes

## ğŸ“‚ Project Structure

Pong Game/
â”œâ”€â”€ Agent/
â”‚ â”œâ”€â”€ agent_pg.py # REINFORCE policy network
â”‚ â”œâ”€â”€ agent_expected_sarsa.py # Q-network for Expected SARSA
â”‚ â””â”€â”€ Paddle.py # Paddle (player) sprite logic
â”‚
â”œâ”€â”€ Env.py # Custom Gym-compatible Pong environment
â”œâ”€â”€ Constants.py # Centralized settings & hyperparameters
â”‚
â”œâ”€â”€ train_pg.py # Train REINFORCE agent
â”œâ”€â”€ train_a2c.py # Train A2C agent
â”œâ”€â”€ train_expected_sarsa.py # Train Expected SARSA agent
â”œâ”€â”€ evaluate.py # Evaluate and compare trained agents
â”‚
â”œâ”€â”€ models/ # Directory for saved model files
â””â”€â”€ Pong_Log/ # TensorBoard logs

## âš™ï¸ Setup

Install required packages:

```bash
pip install pygame torch stable-baselines3 gymnasium matplotlib tensorboard

ğŸ‹ï¸â€â™‚ï¸ Training
Each algorithm has a dedicated training script. Run them as follows:
python train_pg.py               # REINFORCE
python train_a2c.py              # A2C
python train_expected_sarsa.py   # Expected SARSA

ğŸ“Š Visualizing with TensorBoard
tensorboard --logdir Pong_Log/
Then visit: http://localhost:6006
Youâ€™ll see reward curves, loss, and entropy graphs.

 Evaluating Trained Models
python evaluate.py 200k simple
Arguments:
timesteps: same as model name (e.g. 50k, 200k)
mode: simple or complex
The script:
Loads available models
Runs 100 episodes per agent
Prints reward statistics
Saves a reward-vs-episode line plot
Exports results to CSV

âš™ï¸ Hyperparameters (from Constants.py)
| Parameter         | Description                             | Default  |
| ----------------- | --------------------------------------- | -------- |
| `learning_rate`   | Optimizer step size                     | `0.0004` |
| `ent_coef`        | Entropy coefficient (for exploration)   | `0.01`   |
| `gamma`           | Discount factor for future rewards      | `0.95`   |
| `gae_lambda`      | GAE smoothing parameter                 | `0.95`   |
| `max_grad_norm`   | Clipping value for gradients            | `0.5`    |
| `total_timesteps` | Total training time steps per algorithm | `200000` |


ğŸ® Environment Modes
Simple: Standard paddle & ball behavior.
Complex: Paddle shrinks over time, ball speeds up with each bounce.

Reward system:
Hit the ball: +30
Stay aligned with the ball: +1
Get closer to the ball: +0.1
Miss the ball: -30

ğŸ™ Acknowledgments
Built with:
PyTorch
Stable-Baselines3
Gymnasium
Pygame
